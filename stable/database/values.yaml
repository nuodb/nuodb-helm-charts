## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry and imagePullSecrets
##
# global:
#   imageRegistry: myRegistryName
#   imagePullSecrets:
#     - myRegistryKeySecretName

cloud:
  # supported: amazon, azure, google
  provider:
  # zones:
  #   - us-east-2a
  #   - us-east-2b
  #   - us-east-2c

  cluster:
    # cluster name is used to make resources unique in multi-cluster configurations.
    # If the NuoDB domain spans 2 or more physical clusters, then each cluster must have a unique cluster.name.
    # The default is fine for single-cluster domains.
    name: cluster0
    entrypointName: cluster0

busybox:
  image:
    registry: docker.io
    repository: busybox
    tag: latest
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    ##
    # pullSecrets:
    # - myRegistryKeySecretName
    ## Specify a imagePullPolicy
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##

nuodb:
  image:
    registry: docker.io
    repository: nuodb/nuodb
    tag: "6.0"
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    ##
    # pullSecrets:
    # - myRegistryKeySecretName
    ## Specify a imagePullPolicy
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##

  # the prefix for the shared request value - default value is always valid
  requestPrefix: ""

  # the prefix for the shared latest backup value - default value is always valid
  latestPrefix: ""

  # the prefix for the shared latest backup ringbuffer - default value is always valid
  latestKey: ""

  # the name of the ServiceAccount to use for all NuoDB Pods
  serviceAccount: nuodb

admin:
  # domain is the name of the NuoDB administration domain (e.g. the cluster name)
  domain: nuodb

  # namespace: nuodb
  # tlsCACert:
  #   secret: nuodb-ca-cert
  #   key: ca.cert
  # tlsKeyStore:
  #   secret: nuodb-keystore
  #   key: nuoadmin.p12
  #   password: changeIt
  #   passwordKey: password
  # tlsClientPEM:
  #   secret: nuodb-client-pem
  #   key: nuocmd.pem

  serviceSuffix:
    clusterip: "clusterip"
    balancer: "balancer"
    nodeport: "nodeport"

  ## Transparent Data Encryption (TDE) secrets
  # Storage passwords can be managed and stored as Kubernetes
  # secrets. TDE secrets for all databases should be created and
  # provided before deploying NuoDB. For example:
  #   kubectl create secret generic demo-tde-secret -n nuodb \
  #     --from-literal target='topSecret'
  #
  # Note: TDE should be enabled on database layer using
  #   SQL> alter database change encryption type AES128;
  #
  # Storage password rotation is performed by updating the secret and
  # providing the new target and historical (optional) passwords.
  # The historical passwords are used for restoring old backup sets.
  # For example:
  #   kubectl create secret generic demo-tde-secret -n nuodb \
  #     --from-literal target='superSecret' \
  #     --from-literal historical-20201110='topSecret' \
  #     --dry-run=client -o yaml | kubectl apply -f -
  tde:
    secrets: {}
      # demo: demo-tde-secret
    storagePasswordsDir: /etc/nuodb/tde


database:
  ## Provide a name in place of the chart name for `app:` labels
  ##
  #nameOverride: ""

  ## Provide a name to substitute for the full names of resources
  ##
  #fullnameOverride: ""

  # name
  # NuoDB Database name.  must consist of lowercase alphanumeric
  #characters '[a-z0-9]+'
  name: demo

  # rootUser
  # Name of Database user
  rootUser: dba

  # rootPassword
  # Database password
  rootPassword: secret

  # archiveType
  # Archive Type (either "" or "lsa") - for NuoDB Version >= 6.0.0
  archiveType: ""

  # The names of the PriorityClasses that database pods should belong to. For
  # more information on PriorityClasses, see
  # https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
  priorityClasses:
    # The priorityClassName for SM pods (both hotcopy and non-hotcopy SMs).
    sm: ""

    # The priorityClassName for TE pods.
    te: ""

  initContainers:
    # Whether to run init-disk init container to change permissions of mounted
    # volumes. This should not be necessary if fsGroup is being specified in
    # the security context, unless the volumes have a storage class that does
    # not support fsGroup, such as hostpath.
    runInitDisk: true

    # Whether to run init-disk as root. If set to false, the user defined by
    # the Pod security context will be used if one is defined, otherwise the
    # default user for the init-disk container image (busybox) will be used.
    runInitDiskAsRoot: true

    # Kubernetes resource requests and limits set on the init containers
    # Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    resources: {}

  securityContext:
    # Whether to create a security context for Pods containing only the fsGroup
    # value defined below. This is less restrictive than runAsNonRootGroup,
    # because it allows all containers in the Pod, including init and sidecar
    # containers to run using the default user (assuming that the containers
    # does not explicitly specify runAsUser and runAsGroup in their own
    # security contexts).
    fsGroupOnly: false

    # Whether to create a security context for Pods restricting the uid and gid
    # of the runtime "nuodb" user to 1000:1000, which is supported starting
    # with NuoDB image version 4.3.
    #
    # runAsNonRootGroup must be set to false with NuoDB image versions older
    # than 4.3.
    runAsNonRootGroup: false

    # Whether to create a security context for Pods containing the runAsUser
    # and fsGroup values defined below.
    enabled: false

    # runAsUser must be an integer between 1000 and 65533.
    runAsUser: 1000

    # Defining fsGroup in a security context causes volumes mounted into the
    # containers (including those defined by secrets and config maps) to be
    # owned by the specified group ID, which is also added as a supplementary
    # group to the runtime user. This allows a non-root user and group to read,
    # write, and execute files with permissions 440, 660, and 770.
    fsGroup: 1000

    # Whether to create SecurityContext for containers
    enabledOnContainer: false

    # Specify additional container capabilities such as "NET_ADMIN"
    # Object of type Capabilities v1 core
    # ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#capabilities-v1-core
    capabilities:
      add: []
      drop: []

    # Run container in privileged mode. Processes in privileged containers are
    # essentially equivalent to root on the host
    privileged: false

    # Whether a process can gain more privileges than its parent process. This
    # bool directly controls if the "no_new_privs" flag will be set on the
    # container process
    allowPrivilegeEscalation: false

    # Whether to mount the root filesystem as read-only.
    readOnlyRootFilesystem: false

  ## Import Environment Variables inside containers
  # List of EnvVar v1 core definitions
  # ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#envvar-v1-core
  ##
  env: []

  ## Import Environment Variables from one or more configMaps
  # Ex: configMapRef: [ myConfigMap, myOtherConfigMap ]
  ##
  envFrom:
    configMapRef: []

  persistence:
    size: 20Gi
    accessModes:
      - ReadWriteOnce
    # storageClass: "-"

    # The data source for the archive PVC
    # see https://kubernetes.io/docs/concepts/storage/persistent-volumes/#data-source-references
    # schema https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#typedobjectreference-v1-core
    archiveDataSource:
      # name: archive-snapshot
      # namespace: source-namespace
      kind: VolumeSnapshot
      apiGroup: snapshot.storage.k8s.io

    # The data source for the journal PVC
    # see https://kubernetes.io/docs/concepts/storage/persistent-volumes/#data-source-references
    # schema https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#typedobjectreference-v1-core
    journalDataSource:
      # name: journal-snapshot
      # namespace: source-namespace
      kind: VolumeSnapshot
      apiGroup: snapshot.storage.k8s.io

    # Whether to validate that data sources for PVCs exist
    validateDataSources: true

    # Whether to preprovision PVCs from data sources and omit the data source
    # reference from the volumeClaimTemplate section of the statefulset.
    preprovisionVolumes: false

  ## database-wide options.
  # These are applied using the --database-options on the startup command
  # change these to values appropriate for this database
  # these options are applied to all processes in the database.
  options:
    ping-timeout: 60
    max-lost-archives: 0

  # Custom NuoDB configuration files path
  configFilesPath: /etc/nuodb/

  # Custom NuoDB configuration files used to override default NuoDB settings
  configFiles: {}
    # nuodb.config: |-
    #   verbose error,flush,warn

  # Load balancer configuration per database
  lbConfig:
    prefilter: ""
    default: ""

  # ensure all values here are strings - so quote any purely numeric values
  # source should be a tar.gz or an already local backupset.
  # type either stream or backupset
  autoImport:
    source: ""
    credentials: ""
    stripLevels: "1"
    type: ""

  # ensure all values here are strings - so quote any purely numeric values
  autoRestore:
    source: ""
    credentials: ""
    stripLevels: "1"
    type: ""

  # The names of the clusterIP and balancer services can be adjusted by customising the suffix
  # The default is to use the same as specified for the admin clusterIP and balancer load-balancers.
  serviceSuffix:
    clusterip: ""
    balancer: ""
    nodeport: ""

  # One primary and multiple secondary database Helm releases for the same
  # database name may be deployed into the same Kubernetes namespace. Set to
  # `false` to deploy secondary database Helm releases.
  primaryRelease: true

  ephemeralVolume:
    # Whether to enable generic ephemeral volumes, rather than using emptyDir
    # for data that does not have to outlive the pod.
    # ref: https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes
    enabled: false

    # The size of the generic ephemeral volume.
    size: 1Gi

    # Whether to size the volume according to the resources.limits.memory
    # setting of the process, to guarantee that we have enough space to store
    # at least one core file. Since core files are compressed before they hit
    # the disk, it is likely that this setting is sufficient to retain many
    # core files.
    sizeToMemory: false

    # The storage class to use for the generic ephemeral volume.
    # storageClass: "-"

  snapshotRestore:
    # ID to validate archive and journal volumes restored from snapshots
    backupId: ""

    # Template to resolve name of snapshot to use as a data source, where
    # volumeType is one of "archive" or "journal".
    snapshotNameTemplate: "{{.backupId}}-{{.volumeType}}"

  backupHooks:
    # If enabled=true, sidecar containers are created for non-hotcopy SMs to
    # perform synchronization necessary to obtain snapshots of the archive and
    # journal directories.
    enabled: false
    image:
      registry: ghcr.io
      repository: nuodb/nuodb-sidecar
      tag: 1.0.3
      pullPolicy: IfNotPresent

    # Kubernetes resource requests and limits used for the backup hooks sidecar
    # container
    # ref: http://kubernetes.io/docs/user-guide/compute-resources/
    resources: {}

    # Deprecated: use freezeMode="suspend" instead.
    useSuspend: false

    # If the non-hotcopy SM is configured with separate archive and journal
    # volumes, writes are frozen to the archive volume when the pre-backup hook
    # is invoked and unfrozen when the post-backup hook is invoked. Below are
    # supported ways to stop archive writes:
    #   - hotsnap - Uses NuoDB product support for pausing archive writes
    #     (recommended). Supported from NuoDB versions 6.0.2 and above.
    #   - fsfreeze - Uses fsfreeze binary to suspend archive reads and writes.
    #     This mode requires a privileged sidecar container.
    #   - suspend - The SM process is suspended completely and then resumed
    #     using `kill -STOP` and `kill -CONT`. This is to enable usage in
    #     environments where fsfreeze cannot be invoked on the archive
    #     filesystem, e.g. because it is using hostpath-based storage where
    #     other processes would be affected. If `ping-timeout` is enabled, then
    #     the time between the pre and post-backup hook invocations should not
    #     approach the `ping-timeout` value, otherwise there is risk of node
    #     failure detection being triggered and causing the SM to be
    #     shutdown/evicted.
    freezeMode: ""

    # Timeout in seconds until the SM is automatically unfrozen.
    timeout: 30

    # Register scripts to be invoked on HTTP requests. Path parameters can
    # appear in the request path, which are exported as environment variables
    # available within the script. Query parameters are also made available as
    # environment variables. If a request payload is supplied, it can be
    # referenced using the `${payload}` variable. A mapping can also be defined
    # from process exit codes to HTTP status codes. See the examples below.
    customHandlers: []

    #customHandlers:
    #- method: GET
    #  path: /backup-id
    #  script: |
    #    [ -e "$NUODB_ARCHIVE_DIR/backup.txt" ] || exit 1
    #    cat "$NUODB_ARCHIVE_DIR/backup.txt"
    #- method: GET
    #  path: /pid/{command}
    #  script: |
    #    pgrep -x "$command"
    #  statusMappings:
    #    "1": 404
    #- method: POST
    #  path: /operation/{script}/execute
    #  script: |
    #    "$script" $payload
    #  statusMappings:
    #    "0": 200
    #    "1": 400
    #    "*": 500

  podMonitor:
    # Whether to enable PodMonitor resource for database pods.
    enabled: false

    # Labels to assign to the PodMonitor resource. Prometheus must be configured
    # with label selector that matches the defined labels on the resource.
    labels: {}

    # The label to use to retrieve the job name from.
    jobLabel: app

    # The labels which are transferred from the associated Kubernetes Pod object
    # onto the ingested metrics.
    podTargetLabels: []

    # The Pod port name which exposes the endpoint.
    portName: http-metrics

    # Interval at which Prometheus scrapes the metrics from the database pods.
    interval: 10s

    # HTTP path from which to scrape for metrics.
    path: /metrics

    # Timeout after which Prometheus considers the scrape to be failed. If
    # empty, Prometheus uses the global scrape timeout unless it is less than
    # the target’s scrape interval value in which the latter is used.
    scrapeTimeout: ""

    # HTTP scheme to use for scraping.
    scheme: http

    # TLS configuration to use when scraping the target. See
    # https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1.SafeTLSConfig
    tlsConfig: {}

    # The relabeling rules to apply to the samples before ingestion. See
    # https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1.RelabelConfig
    relabelings: []

    # The relabeling rules to apply to the samples before ingestion. See
    # https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1.RelabelConfig
    metricRelabelings: []

    # Configures the Basic Authentication credentials to use when scraping. See
    # https://prometheus-operator.dev/docs/api-reference/api/#monitoring.coreos.com/v1.BasicAuth
    basicAuth: {}

  sm:
    ## Enable persistent log volumes to retain logs when an external logging solution is not used.
    logPersistence:
      enabled: false
      overwriteBackoff:
        # Copies of the crash directory are taken to avoid overwrites of root crash.
        # This controls the window within which no more copies of the crash directory will be taken to avoid the disk filling.
        # Default will retain 3 copies within the last 120 minutes, after which copies will continue to be created.
        copies: 3
        windowMinutes: 120
      size: 60Gi
      accessModes:
        - ReadWriteOnce
      # storageClass: "-"

    # Settings for storage manager (SM) nodes with hotcopy enabled.
    # Total SM Limit is 1 in CE version of NuoDB
    # These SMs have hotcopy backup enabled. To start SMs without hotcopy use
    # database.sm.noHotCopy.replicas
    # All time values are in seconds unless the unit is included in the name.
    hotCopy:
      enablePod: true
      enableBackups: true
      replicas: 1

      # Deadline for starting a hotcopy job - if a job hasn't started in this time - give up
      deadline: 1800

      # timeout for completing a full or incremental hotcopy job - if a job
      # hasn't completed in this time - stop waiting for it; the default timeout
      # of "0" will force the backup jobs to wait forever for the requested
      # hotcopy operation to complete
      timeout: 0

      successHistory: 5
      failureHistory: 5

      # Kubernetes resource requests and limits set on the backup jobs;
      # Ref: http://kubernetes.io/docs/user-guide/compute-resources/
      jobResources: {}

      backupDir: /var/opt/nuodb/backup

      # automatically generated backup groups will have this prefix - defaults
      # to cloud.cluster.name
      backupGroupPrefix: ""

      # hot-copy operation is requested separately per backup group; if not
      # configured, a backup group per HCSM is created automatically to ensure
      # that the configured backup redundancy is met; the automatically
      # generated backup group name of "<backupGroupPrefix>-<ordinal>" is used
      # where the ordinal matches the HCSM ordinal; if the database is split
      # across archives (storage groups are in use), each backup group should
      # have a minimum set of archives that together contain the entire contents
      # of the database
      backupGroups: {}
      #  cluster0-0:
      #    labels: "pod-name sm-database-nuodb-cluster0-demo-hotcopy-0"
      #    processFilter: "label(pod-name sm-database-nuodb-cluster0-demo-hotcopy-0)"
      #    fullSchedule: "35 22 * * 6"
      #    incrementalSchedule: "35 22 * * 0-5"
      #    journalSchedule: "?/15 * * * *"


      ## Schedule for the running of the full backup cron job
      # Eg: At 22:35 on every day-of-month that is a Sunday
      fullSchedule: "35 22 * * 6"

      ## Schedule for the running the incremental backup cron job
      # Eg: At 22:35 on every day-of-month on every day-of-week from Monday through Saturday
      incrementalSchedule: "35 22 * * 0-5"

      restartPolicy: OnFailure

      persistence:
        size: 20Gi
        accessModes:
          - ReadWriteOnce
        # storageClass: "-"

      journalPath:
        enabled: false
        persistence:
          size: 20Gi
          accessModes:
            - ReadWriteOnce
          # storageClass: "-"

      coldStorage:
        credentials: ""

      ## settings for the journal backup
      journalBackup:
        enabled: false
        # schedule for the running the journal backup cron job; when journal
        # backup is enabled, an SM will retain each journal file on disk until
        # it is journal hot copied into a backup set; this means that journal
        # hot copy must be executed periodically to prevent SMs from running out
        # of disk space on the journal volume
        journalSchedule: "?/15 * * * *"
        deadline: 90
        # timeout for completing a journal hotcopy job - if a job hasn't
        # completed in this time - stop waiting for it; the default timeout of
        # "0" will force the backup jobs to wait forever for
        # the requested hotcopy operation to complete
        timeout: 0

    # Number of storage manager (SM) nodes that do not have hotcopy backup enabled.
    # SM Limit is 1 in CE version of NuoDB
    # These SMs do not have hotcopy enabled, to start SMs with hotcopy use
    # database.sm.HotCopy.replicas
    noHotCopy:
      enablePod: true
      replicas: 0

      journalPath:
        enabled: false
        persistence:
          size: 20Gi
          accessModes:
            - ReadWriteOnce
          # storageClass: "-"

    ## resources
    # k8s resource min (request) and max (limit)
    # min is also used for the target maximum memory used by the cache (NuoDB --mem option)
    resources:
      limits:
        cpu: 8
        memory: 16Gi
      requests:
        cpu: 4
        memory: 8Gi

    ## Affinity, selector, and tolerations
    # They are expanded as YAML, and can include variable and template references
    affinity: {}
    # nodeSelector: {}
    # tolerations: []
    # topologySpreadConstraints: []

    ## labels
    # Additional Labels given to the SMs started
    labels: {}

    # additional options to pass to the starting NuoDB process.
    engineOptions: {}

    # named key/value pairs that need to be passed to the image, such as
    # keystore: "/etc/nuodb/keys/nuoadmin.p12"
    otherOptions: {}

    # Deprecated: Use readinessProbe.timeoutSeconds instead
    readinessTimeoutSeconds: 5

    # Configuration for readiness probe, which controls ready status and whether
    # the pod has traffic from Services dispatched to it.
    readinessProbe:

      # By default, use 5-second delay/period and mark container as not ready
      # after three failures.
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 3
      successThreshold: 1

      # The readiness probe for engines only inspects the /proc filesystem, so
      # it should not take longer than 5 seconds. This is commented out to
      # enable fall-back to the deprecated readinessTimeoutSeconds value.
      #timeoutSeconds: 5

    # Table Partitions and Storage Groups (TP/SG) is the mechanism by which a
    # version of an SQL table row is stored only on a subset of Storage Managers
    # (SMs). The "storageGroup" section configures the named Storage Group (SG)
    # which the SMs installed with this Helm release will initially serve when
    # their archives are created. The "UNPARTITIONED" storage group is
    # implicitly added as it is served by every SM in the database. The
    # information about served SGs is stored in the archive and may change over
    # time. The SM will emit warning messages if there is a mismatch between the
    # initially configured SG and the currently served SGs.
    storageGroup:
      enabled: false
      # The name of the storage group. Only alphanumeric and underscore ('_')
      # characters are allowed. By default the Helm release name is used.
      name: ""

  te:
    # Provision Transaction Engine (TE) Deployment. By default, the TE
    # Deployment is disabled if TP/SG is enabled and this is a "secondary"
    # release.
    #
    # enablePod: true

    ## Enable persistent log volumes to retain logs when an external logging solution is not used.
    logPersistence:
      enabled: false
      overwriteBackoff:
        # Copies of the crash directory are taken to avoid overwrites of root crash.
        # This controls the window within which no more copies of the crash directory will be taken to avoid the disk filling.
        # Default will retain 3 copies within the last 120 minutes, after which copies will continue to be created.
        copies: 3
        windowMinutes: 120
      size: 60Gi
      accessModes:
        - ReadWriteMany
      # storageClass: "-"

    ## Enable the Layer 4 Load balancer if required,
    ## and specify if it should provision an internal or external cloud IP address
    externalAccess: {}
      # enabled: false
      # internalIP: true
      # type: LoadBalancer
      # annotations: {}

    # Enable NuoDB external access via Ingress; the provisioned Ingress
    # controller should support SSL passthrough feature allowing it to send the
    # TLS connections directly to the Transaction Engines instead of decrypting
    # the communication; supported starting with Kubernetes v1.19.0 and NuoDB
    # image version 4.2.3
    ingress:
      enabled: false
      # the fully qualified domain name (FQDN) of the network host for SQL
      # clients
      hostname: ""
      # For Kubernetes >= 1.18 the ingress class name should be specified via the
      # ingressClassName option instead of using annotation
      className: ""
      # custom annotations that are set on the Ingress resource; SSL passthrough
      # feature should be configured so that SQL clients TLS connections are
      # send directly to the Transaction Engines
      annotations:
        ingress.kubernetes.io/ssl-passthrough: "true"

    ## By default, the database clusterip service for direct TE connections is enabled,
    ## but can be optionally disabled here
    dbServices: {}
      # enabled: false

    # Number of Transaction Engine (TE) replicas. A non-zero value is discarded
    # if TE autoscaling is enabled.
    replicas: 1

    ## resources
    # k8s resource min (request) and max (limit)
    # min is also used for the target maximum memory used by the cache (NuoDB --mem option)
    resources:
      limits:
        cpu: 4
        memory: 16Gi
      requests:
        cpu: 2
        memory: 8Gi

    ## Affinity, selector, and tolerations
    # There are expanded as YAML, and can include variable and template references
    affinity: {}
    # nodeSelector: {}
    # tolerations: []
    # topologySpreadConstraints: []

    # labels
    # Additional Labels given to the TEs started
    labels: {}

    # additional options to pass to the starting NuoDB process.
    engineOptions: {}

    # named key/value pairs that need to be passed to the image, such as
    # keystore: "/etc/nuodb/keys/nuoadmin.p12"
    otherOptions: {}

    # Deprecated: Use readinessProbe.timeoutSeconds instead
    readinessTimeoutSeconds: 5

    # Configuration for readiness probe, which controls ready status and whether
    # the pod has traffic from Services dispatched to it.
    readinessProbe:

      # By default, use 5-second delay/period and mark container as not ready
      # after three failures.
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 3
      successThreshold: 1

      # The readiness probe for engines only inspects the /proc filesystem, so
      # it should not take longer than 5 seconds. This is commented out to
      # enable fall-back to the deprecated readinessTimeoutSeconds value.
      #timeoutSeconds: 5

    ## Configures auto-scaling (scale-out) for TE deployment.
    autoscaling:
      # The lower limit for the number of TE replicas to which the autoscaler
      # can scale down.
      minReplicas: 1

      # The upper limit for the number of TE replicas to which the autoscaler
      # can scale up. It cannot be less than the minReplicas.
      maxReplicas: 3

      # Configuration for the HorizontalPodAutoscaler resource.
      hpa:
        # Whether to enable auto-scaling for TE deployment by using HPA
        # resource.
        enabled: false

        # The target average CPU utilization value across all TE pods,
        # represented as a percentage.
        targetCpuUtilization: 80

        # Configures the scaling behavior of the target in both Up and Down
        # directions. See:
        # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.33/#horizontalpodautoscalerbehavior-v2-autoscaling
        behavior:
          scaleUp:
            # The number of seconds for which past recommendations should be
            # considered while scaling up.
            stabilizationWindowSeconds: 300

        # Custom annotations set on the HPA resource
        annotations: {}

      # Configuration for the ScaledObject resource managed by KEDA.
      keda:
        # Whether to enable auto-scaling for TE deployment by using KEDA
        # ScaledObject resource.
        enabled: false

        # The interval in seconds to check each trigger on.
        pollingInterval: 30

        # The period in seconds to wait after the last trigger reported active
        # before scaling the resource back to 0.
        cooldownPeriod: 300

        # The number of replicas to fall back to if a scaler is in an error
        # state. See: https://keda.sh/docs/latest/reference/scaledobject-spec/
        fallback: {}

        # List of triggers to activate scaling of the target resource. See:
        # https://keda.sh/docs/latest/scalers/
        triggers: []

        # Custom annotations set on the ScaledObject resource
        annotations: {}

  # These annotations will pass through to the pod as supplied, useful for integrating 3rd party products such as Vault.
  podAnnotations: {}
    # vault.hashicorp.com/agent-inject: true

  # Set to true if you are using manually created volumes or restoring
  # from a previously existing backup.
  isManualVolumeProvisioning: false
  isRestore: false

  # A new version of NuoDB database software may also introduce a new version of
  # the database protocol. NuoDB supports explicit database protocol version
  # upgrade which is performed after upgrading the NuoDB image for all database
  # processes. To simplify NuoDB version rollout, Kubernetes Aware Admin (KAA)
  # is used to automatically upgrade database protocol and restart a Transaction
  # Engine (TE) as an upgrade finalization step. For more information on how to
  # perform the steps manually, please check Upgrading the Database Protocol
  # (https://doc.nuodb.com/nuodb/latest/deployment-models/physical-or-vmware-environments-with-nuodb-admin/installing-nuodb/upgrading-to-a-new-release/upgrading-the-database-protocol/).
  #
  # IMPORTANT: After the database protocol version has been upgraded, the NuoDB
  # Archive cannot be used with NuoDB software versions that only support the
  # previous database protocol version. As a result, downgrading after the
  # database protocol version has been changed will require restoring a backup
  # of the database.
  automaticProtocolUpgrade:
    # controls if an automatic protocol upgrade should be done for this database
    enabled: false
    # LBQuery expression to select the TE that will be restarted after
    # successful database protocol upgrade. Defaults to random TE. For more
    # information, please check LBQuery Expression Syntax
    # (https://doc.nuodb.com/nuodb/latest/client-development/load-balancer-policies/#_lbquery_expression_syntax).
    tePreferenceQuery: ""

  # Custom labels attached to the Kubernetes resources installed by this Helm
  # Chart; the labels are immutable and can't be changed with Helm upgrade; for
  # allowed syntax and character sets, please check
  # https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set
  resourceLabels: {}

  legacy:
    headlessService:
      enabled: false
    directService:
      enabled: false

nuocollector:
  # Enable NuoDB Collector by setting nuocollector.enabled=true
  enabled: false
  image:
    registry: docker.io
    repository: nuodb/nuodb-collector
    tag: 2.0.0
    pullPolicy: IfNotPresent
  watcher:
    # Whether to enable NuoDB Collector configuration watcher. If disabled,
    # desired output plugins must be defined during the installation of the
    # NuoDB Helm Charts release
    enabled: true
    registry: ghcr.io
    repository: nuodb/nuodb-sidecar
    tag: 1.0.3
    pullPolicy: IfNotPresent

  # Kubernetes resource requests and limits used for the nuocollector sidecar
  # container
  # ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}

  ## Import environment variables inside nuocollector container. See:
  ## https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#envvar-v1-core
  ##
  env: []

  # Ports to expose on the nuocollector container. See
  # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#containerport-v1-core
  ports: []

  plugins:
    ## NuoDB Collector compatible plugins specific for database services
    database: {}
