// +build diagnostics

package minikube

import (
	"errors"
	"fmt"
	"github.com/Masterminds/semver"
	"github.com/gruntwork-io/terratest/modules/helm"
	"github.com/gruntwork-io/terratest/modules/k8s"
	"github.com/nuodb/nuodb-helm-charts/v3/test/testlib"
	"github.com/stretchr/testify/require"
	"io/ioutil"
	"regexp"
	"strings"

	corev1 "k8s.io/api/core/v1"
	"testing"
	"time"
)

func verifyKillAndInfoInLog(t *testing.T, namespaceName string, adminPodName string, podName string) {
	kubectlOptions := k8s.NewKubectlOptions("", "", namespaceName)
	previousCount := testlib.GetPodRestartCount(t, namespaceName, podName)

	ch := make(chan string)
	// make sure to collect the old logs
	go func() {
		ch <- testlib.GetAppLog(t, namespaceName, podName, "-previous", &corev1.PodLogOptions{Follow: true})
	}()

	// send SIGABRT
	k8s.RunKubectl(t, kubectlOptions, "exec", podName, "--", "kill", "-6", "1")

	// dumping core can take a long time, give it a long timeout
	testlib.AwaitPodRestartCountGreaterThan(t, namespaceName, podName, previousCount, 300*time.Second)

	testlib.AwaitPodUp(t, namespaceName, podName, 100*time.Second)
	testlib.AwaitDatabaseUp(t, namespaceName, adminPodName, "demo", 2)

	oldPodLogFileName := <-ch
	buf, err := ioutil.ReadFile(oldPodLogFileName)
	require.NoError(t, err)
	fullLog := string(buf)
	require.Greater(t, strings.Count(fullLog, "Core was generated by"), 0, "Could not find core parsing in log file, full log: %s", fullLog)

	// check that the core was moved to a dated crash directory for managing the number of core dumps
	output, err := k8s.RunKubectlAndGetOutputE(t, kubectlOptions, "exec", podName, "--", "find", "/var/log/nuodb/", "-type", "d")
	require.NoError(t, err, output)
	require.Regexp(t, regexp.MustCompile("crash-[0-9]{8}T[0-9]{6}"), output)

	testlib.RunOnNuoDBVersionCondition(t, ">4.2.2", func(version *semver.Version) {
		// check that threads from the core are dumped and sent to the admin
		require.Equal(t, 1, testlib.GetStringOccurrenceInLog(t, namespaceName, adminPodName,
			fmt.Sprintf("[%s:%s] STDOUT --- Uncompressing core file ...", adminPodName, podName), &corev1.PodLogOptions{}),
			"Unable to find %s core dump threads in NuoAdmin log file", podName)
		t.Log("Finished checking code dump threads")
	})
}

func TestKubernetesPrintCores(t *testing.T) {
	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)

	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t, &helm.Options{}, 1, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	databaseHelmChartReleaseName := testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.sm.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.sm.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.te.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.logPersistence.enabled":    "true",
			"database.sm.logPersistence.enabled":    "true",
			"database.options.ping-timeout":         "0", // disable network fault handling
		},
	})

	t.Run("killTEWithCore", func(t *testing.T) {
		tePodNameTemplate := fmt.Sprintf("te-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
		tePodName := testlib.GetPodName(t, namespaceName, tePodNameTemplate)
		verifyKillAndInfoInLog(t, namespaceName, admin0, tePodName)

		testlib.RecoverCoresFromEngine(t, namespaceName, "te", "demo-log-te-volume")
	})

	t.Run("killSMWithCore", func(t *testing.T) {
		smPodTemplate := fmt.Sprintf("sm-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
		smPodName := testlib.GetPodName(t, namespaceName, smPodTemplate)
		verifyKillAndInfoInLog(t, namespaceName, admin0, smPodName)

		smLogPvc := fmt.Sprintf("log-volume-%s", smPodName)
		testlib.RecoverCoresFromEngine(t, namespaceName, "sm", smLogPvc)

		testlib.RunOnNuoDBVersionCondition(t, ">4.2.2", func(version *semver.Version) {
			// check that logging from nuosm script is sent to the admin
			require.GreaterOrEqual(t, testlib.GetStringOccurrenceInLog(t, namespaceName, admin0,
				fmt.Sprintf("[:%s] nuosm ==", smPodName), &corev1.PodLogOptions{}), 1,
				"Unable to find nuosm logging in NuoAdmin log")
			t.Log("Finished checking nuosm logging")
		})
	})
}

func TestPermanentLossOfAdmin(t *testing.T) {
	t.Skip("pod killing is flaky!")

	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)

	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t,
		&helm.Options{SetValues: map[string]string{"admin.replicas": "3"}},
		3, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	databaseHelmChartReleaseName := testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.sm.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.sm.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.te.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
		},
	})

	tePodNameTemplate := fmt.Sprintf("te-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
	tePodName := testlib.GetPodName(t, namespaceName, tePodNameTemplate)

	adminToKill, err := GetAdminOfEnginePodE(t, namespaceName, admin0, tePodName)
	require.NoError(t, err)

	if adminToKill == admin0 {
		t.Skip("Can not delete storage of entry node admin-0. Abandoning test.")
	}

	kubectlOptions := k8s.NewKubectlOptions("", "", namespaceName)

	k8s.RunKubectl(t, kubectlOptions, "exec", adminToKill, "--", "rm", "-rf", "/var/opt/nuodb/admin*")

	verifyPodKill(t, namespaceName, adminToKill, helmChartReleaseName, 3)

	testlib.AwaitDatabaseUp(t, namespaceName, admin0, "demo", 2)

	testlib.Await(t, func() bool {
		return testlib.GetStringOccurrenceInLog(t, namespaceName, adminToKill,
			"Reconnected with process with connectKey", &corev1.PodLogOptions{}) >= 1
	}, 30*time.Second)

}

func GetAdminOfEnginePodE(t *testing.T, namespaceName string, admin0 string, podName string) (string, error) {
	kubectlOptions := k8s.NewKubectlOptions("", "", namespaceName)

	output, err := k8s.RunKubectlAndGetOutputE(t, kubectlOptions, "exec", admin0, "--",
		"nuocmd", "--show-json", "get", "processes", "--db-name", "demo")

	require.NoError(t, err, output)

	err, objects := testlib.Unmarshal(output)

	for _, obj := range objects {
		if obj.Hostname == podName {
			t.Logf("found %s on %s", obj.Hostname, obj.Host)
			return obj.Host, nil
		}
	}

	return "", errors.New("GetAdminOfEnginePodE: expected pod was not found in 'get processes'")
}

func TestReadWriteManyEnabledManyEngines(t *testing.T) {
	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)

	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t, &helm.Options{}, 1, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.sm.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.sm.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.resources.requests.cpu":    "250m",
			"database.te.resources.requests.memory": "250Mi",
			"database.te.logPersistence.enabled":    "true",
			"database.te.replicas":                  "2",
		},
	})
}

func TestReadWriteManyMultitenancy(t *testing.T) {
	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)

	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t, &helm.Options{}, 1, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.name":                         "green",
			"database.sm.resources.requests.cpu":    "250m",
			"database.sm.resources.requests.memory": "250Mi",
			"database.te.resources.requests.cpu":    "250m",
			"database.te.resources.requests.memory": "250Mi",
			"database.te.logPersistence.enabled":    "true",
		},
	})

	testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.name":                         "blue",
			"database.sm.resources.requests.cpu":    "250m",
			"database.sm.resources.requests.memory": "250Mi",
			"database.te.resources.requests.cpu":    "250m",
			"database.te.resources.requests.memory": "250Mi",
			"database.te.logPersistence.enabled":    "true",
		},
	})
}

func TestNuoDBKubeDiagnostics(t *testing.T) {
	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)
	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t, &helm.Options{}, 1, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	databaseHelmChartReleaseName := testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.sm.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.sm.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.te.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
		},
	})

	config := testlib.GetNuoDBK8sConfigDump(t, namespaceName, admin0)

	require.True(t, func() bool { _, ok := config.Pods[admin0]; return ok }())

	tePodNameTemplate := fmt.Sprintf("te-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
	tePodName := testlib.GetPodName(t, namespaceName, tePodNameTemplate)
	require.True(t, func() bool { _, ok := config.Pods[tePodName]; return ok }())

	smPodTemplate := fmt.Sprintf("sm-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
	smPodName := testlib.GetPodName(t, namespaceName, smPodTemplate)
	require.True(t, func() bool { _, ok := config.Pods[smPodName]; return ok }())
}

func TestGetDiagnoseInfoAdminLogPersistence(t *testing.T) {
	// start an admin with log persistence enabled
	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)
	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t, &helm.Options{
		SetValues: map[string]string{
			"admin.logPersistence.enabled": "true",
		},
	}, 1, "")

	options := k8s.NewKubectlOptions("", "", namespaceName)
	admin := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	// simulate the creation of a real filesystem by creating a lost+found
	// directory; with certain PV provisioners, the directory is owned by
	// root and has permissions 700 (not group-writable); remove all access
	// (read or write) to make the file inaccessible when streaming the
	// contents of NUODB_LOGDIR
	k8s.RunKubectl(t, options, "exec", admin, "--", "mkdir", "/var/log/nuodb/lost+found")
	k8s.RunKubectl(t, options, "exec", admin, "--", "chmod", "-rwx", "/var/log/nuodb/lost+found")
	output, err := k8s.RunKubectlAndGetOutputE(t, options, "exec", admin, "--", "stat", "-c", "%a", "/var/log/nuodb/lost+found")
	require.NoError(t, err, output)
	require.Equal(t, "0", strings.TrimSpace(output))

	// invoke 'nuocmd get diagnose-info', which should download the contents
	// of NUODB_LOGDIR as a ZIP file, unzip the file, and repackage it with
	// some other diagnostic info; this can fail either when unzipping the
	// file, or while streaming the response; in newer versions of the
	// product we skip inaccessible files rather than failing the request,
	// so check for presence of logging to decide if the request should have
	// failed
	output, _ = k8s.RunKubectlAndGetOutputE(t, options, "exec", admin, "--", "nuocmd", "get", "diagnose-info", "--include-cores", "--output-dir", "/tmp")
	msgCount := testlib.GetStringOccurrenceInLog(t, namespaceName, admin,
		"Unable to package file /var/log/nuodb/lost+found in ZIP file", &corev1.PodLogOptions{})
	if msgCount == 0 {
		require.True(t, strings.Contains(output, "Unable to download diagnose info for "+admin))
	} else {
		require.Equal(t, 1, msgCount)
	}

	// kill the admin pod and wait for it to be re-created
	verifyPodKill(t, namespaceName, admin, helmChartReleaseName, 1)

	// check that the lost+found directory has the correct permissions
	output, err = k8s.RunKubectlAndGetOutputE(t, options, "exec", admin, "--", "stat", "-c", "%a", "/var/log/nuodb/lost+found")
	require.NoError(t, err, output)
	require.Equal(t, "770", strings.TrimSpace(output))

	// check that 'nuocmd get diagnose-info' does not generate any errors or
	// logging about inaccessible files
	output, _ = k8s.RunKubectlAndGetOutputE(t, options, "exec", admin, "--", "nuocmd", "get", "diagnose-info", "--include-cores", "--output-dir", "/tmp")
	msgCount = testlib.GetStringOccurrenceInLog(t, namespaceName, admin,
		"Unable to package file /var/log/nuodb/lost+found in ZIP file", &corev1.PodLogOptions{})
	require.Equal(t, 0, msgCount)
	require.False(t, strings.Contains(output, "Unable to download diagnose info for "+admin))
}
