// +build diagnostics

package minikube

import (
	"errors"
	"fmt"
	"github.com/gruntwork-io/terratest/modules/helm"
	"github.com/gruntwork-io/terratest/modules/k8s"
	"github.com/nuodb/nuodb-helm-charts/test/testlib"
	"github.com/stretchr/testify/require"
	"io/ioutil"
	"regexp"
	"strings"

	corev1 "k8s.io/api/core/v1"
	"testing"
	"time"
)

func verifyKillAndInfoInLog(t *testing.T, namespaceName string, adminPodName string, podName string) {
	options := k8s.NewKubectlOptions("", "", namespaceName)

	previousCount := testlib.GetPodRestartCount(t, namespaceName, podName)

	ch := make(chan string)
	// make sure to collect the old logs
	go func() {
		ch <- testlib.GetAppLog(t, namespaceName, podName, "-previous", &corev1.PodLogOptions{Follow: true})
	}()

	// send SIGABRT
	k8s.RunKubectl(t, options, "exec", podName, "--", "kill", "-6", "1")

	// dumping core can take a long time, give it a long timeout
	testlib.AwaitPodRestartCountGreaterThan(t, namespaceName, podName, previousCount, 300*time.Second)

	testlib.AwaitPodUp(t, namespaceName, podName, 100*time.Second)
	testlib.AwaitDatabaseUp(t, namespaceName, adminPodName, "demo", 2)

	oldPodLogFileName := <- ch
	buf, err := ioutil.ReadFile(oldPodLogFileName)
	require.NoError(t, err)
	fullLog := string(buf)
	require.Greater(t, strings.Count(fullLog, "Core was generated by"), 0, "Could not find core parsing in log file, full log: %s", fullLog)

	// check that the core was moved to a dated crash directory for managing the number of core dumps
	kubectlOptions := k8s.NewKubectlOptions("", "", namespaceName)
	output, err := k8s.RunKubectlAndGetOutputE(t, kubectlOptions, "exec", podName, "--", "find", "/var/log/nuodb/", "-type", "d")
	require.NoError(t, err, output)
	require.Regexp(t, regexp.MustCompile("crash-[0-9]{8}T[0-9]{6}") ,output)
}

func TestKubernetesPrintCores(t *testing.T) {
	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)

	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t, &helm.Options{}, 1, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	databaseHelmChartReleaseName := testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.sm.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.sm.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.te.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.logPersistence.enabled": 	 "true",
			"database.sm.logPersistence.enabled": 	 "true",
			"database.options.ping-timeout": 		 "0", // disable network fault handling
		},
	})

	t.Run("killTEWithCore", func(t *testing.T) {
		tePodNameTemplate := fmt.Sprintf("te-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
		tePodName := testlib.GetPodName(t, namespaceName, tePodNameTemplate)
		verifyKillAndInfoInLog(t, namespaceName, admin0, tePodName)

		testlib.RecoverCoresFromEngine(t, namespaceName, "te", "demo-log-te-volume")
	})

	t.Run("killSMWithCore", func(t *testing.T) {
		smPodTemplate := fmt.Sprintf("sm-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
		smPodName := testlib.GetPodName(t, namespaceName, smPodTemplate)
		verifyKillAndInfoInLog(t, namespaceName, admin0, smPodName)
		
		smLogPvc := fmt.Sprintf("log-volume-%s", smPodName)
		testlib.RecoverCoresFromEngine(t, namespaceName, "sm", smLogPvc)
	})
}

func TestPermanentLossOfAdmin(t *testing.T) {
	t.Skip("pod killing is flaky!")

	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)

	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t,
		&helm.Options{SetValues: map[string]string{"admin.replicas":  "3"}},
		3, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	databaseHelmChartReleaseName := testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.sm.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.sm.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.te.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
		},
	})

	tePodNameTemplate := fmt.Sprintf("te-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
	tePodName := testlib.GetPodName(t, namespaceName, tePodNameTemplate)

	adminToKill, err := GetAdminOfEnginePodE(t, namespaceName, admin0, tePodName)
	require.NoError(t, err)

	if adminToKill == admin0 {
		t.Skip("Can not delete storage of entry node admin-0. Abandoning test.")
	}

	kubectlOptions := k8s.NewKubectlOptions("", "", namespaceName)

	k8s.RunKubectl(t, kubectlOptions, "exec", adminToKill, "--", "rm", "-rf", "/var/opt/nuodb/admin*")

	verifyPodKill(t, namespaceName, adminToKill, helmChartReleaseName, 3)

	testlib.AwaitDatabaseUp(t, namespaceName, admin0, "demo", 2)

	testlib.Await(t, func() bool {
		return testlib.GetStringOccurrenceInLog(t, namespaceName, adminToKill,
			"Reconnected with process with connectKey", &corev1.PodLogOptions{} ) >= 1
	}, 30*time.Second)

}

func GetAdminOfEnginePodE(t *testing.T, namespaceName string, admin0 string, podName string) (string, error) {
	kubectlOptions := k8s.NewKubectlOptions("", "", namespaceName)

	output, err := k8s.RunKubectlAndGetOutputE(t, kubectlOptions, "exec", admin0, "--",
		"nuocmd", "--show-json", "get", "processes", "--db-name", "demo")

	require.NoError(t, err, output)

	err, objects := testlib.Unmarshal(output)

	for _, obj := range objects {
		if obj.Hostname == podName {
			t.Logf("found %s on %s", obj.Hostname, obj.Host)
			return obj.Host, nil
		}
	}

	return "", errors.New("GetAdminOfEnginePodE: expected pod was not found in 'get processes'")
}

func TestReadWriteManyEnabledManyEngines(t *testing.T) {
	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)

	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t, &helm.Options{}, 1, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.sm.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.sm.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.resources.requests.cpu":    "250m",
			"database.te.resources.requests.memory": "250Mi",
			"database.te.logPersistence.enabled":    "true",
			"database.te.replicas":                  "2",
		},
	})
}

func TestReadWriteManyMultitenancy(t *testing.T) {
	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)

	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t, &helm.Options{}, 1, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.name":                         "green",
			"database.sm.resources.requests.cpu":    "250m",
			"database.sm.resources.requests.memory": "250Mi",
			"database.te.resources.requests.cpu":    "250m",
			"database.te.resources.requests.memory": "250Mi",
			"database.te.logPersistence.enabled":    "true",
		},
	})

	testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.name":                         "blue",
			"database.sm.resources.requests.cpu":    "250m",
			"database.sm.resources.requests.memory": "250Mi",
			"database.te.resources.requests.cpu":    "250m",
			"database.te.resources.requests.memory": "250Mi",
			"database.te.logPersistence.enabled":    "true",
		},
	})
}

func TestNuoDBKubeDiagnostics(t *testing.T) {
	testlib.AwaitTillerUp(t)
	defer testlib.VerifyTeardown(t)
	defer testlib.Teardown(testlib.TEARDOWN_ADMIN)

	helmChartReleaseName, namespaceName := testlib.StartAdmin(t, &helm.Options{}, 1, "")

	admin0 := fmt.Sprintf("%s-nuodb-cluster0-0", helmChartReleaseName)

	defer testlib.Teardown(testlib.TEARDOWN_DATABASE) // ensure resources allocated in called functions are released when this function exits

	databaseHelmChartReleaseName := testlib.StartDatabase(t, namespaceName, admin0, &helm.Options{
		SetValues: map[string]string{
			"database.sm.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.sm.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
			"database.te.resources.requests.cpu":    testlib.MINIMAL_VIABLE_ENGINE_CPU,
			"database.te.resources.requests.memory": testlib.MINIMAL_VIABLE_ENGINE_MEMORY,
		},
	})

	config := testlib.GetNuoDBK8sConfigDump(t, namespaceName, admin0)

	require.True(t, func() bool { _, ok := config.Pods[admin0]; return ok }())

	tePodNameTemplate := fmt.Sprintf("te-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
	tePodName := testlib.GetPodName(t, namespaceName, tePodNameTemplate)
	require.True(t, func() bool { _, ok := config.Pods[tePodName]; return ok }())

	smPodTemplate := fmt.Sprintf("sm-%s-nuodb-%s-%s", databaseHelmChartReleaseName, "cluster0", "demo")
	smPodName := testlib.GetPodName(t, namespaceName, smPodTemplate)
	require.True(t, func() bool { _, ok := config.Pods[smPodName]; return ok }())
}
